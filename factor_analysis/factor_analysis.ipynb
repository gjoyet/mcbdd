{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287d8e99",
   "metadata": {},
   "source": [
    "\n",
    "## Factor Analysis\n",
    "\n",
    "#### What is factor analysis?\n",
    "\n",
    "Factor analysis (FA) is the idea of clustering variables of some observed data into a smaller number of so-called factors while still accounting for as much variability as possible. FA does this by exploiting correlations between the variables, assuming that underlying latent variables determine those variables contained in the same factor. It is a method for dimensionality reduction and thus reduces noise, making comparisons between data points easier and thus allowing for applications like facial recognition, anomaly detection, and so on.\n",
    "\n",
    "\n",
    "#### What are the relationships between covariance matrix, factor analysis, and principal component analysis (PCA)?\n",
    "\n",
    "PCA is one possible way to perform factor analysis, although they are not exactly the same. When performing PCA on data in feature space, the principal components will be composites of variables which tend to be correlated, hence telling you which variables could be put into the same cluster.\n",
    "\n",
    "One way to perform PCA in by calculating the covariance matrix of your data and then its eigendecomposition: the eigenvectors will be the PCs while the eigenvalues are the variance along the PCs. This makes intuitive sense, too: since you are trying to capture the most variability in the data in as little dimensions as possible, taking the directions with most explained variance first seems to be a sound method.\n",
    "\n",
    "\n",
    "#### What do we mean with loadings?\n",
    "\n",
    "The loading of a variable for a certain factor is the weight of that variable in that factor. When comparing PCA to factor analysis, the loading of variable i for factor j would correspond to the i-th entry of the j-th eigenvector.\n",
    "\n",
    "\n",
    "#### Why factors are orthogonal to each other? Whatâ€™s the consequence?\n",
    "\n",
    "In PCA, the PCs are orthogonal to each other because that is a constraint we set for PCA. Mathematically, they are orthogonal because the eigenvectors of a symmetric matrix (a covariance matrix is always symmetric since $Cov(x,y) = Cov(y,x)$) are orthogonal to each other. In FA, generally, I believe factors are not required to be orthogonal (or at least the Wikipedia article does not say so).  \n",
    "\n",
    "The consequence of them being orthogonal is that the representation of the observations in the space spanned by the factors is unique, which I think would not be the case if they were linearly dependent.\n",
    "\n",
    "\n",
    "#### How can we use factor analysis as a generative model?\n",
    "\n",
    "You could generate new samples by sampling a factor vector from a multivariate gaussian with $\\mu = \\text{mean of the observed data in the space spanned by the factors}$, which will usually produce more realistic samples than if sampled directly from the initial feature space since it will be less noisy.\n",
    "\n",
    "\n",
    "#### What is the relationship between factor analysis and autoencoder?\n",
    "\n",
    "An autoencoder is the neural network analogue to factor analysis. It is also a method for dimensionality reduction and can be used to eliminate noise and compare data points. However, with an autoencoder, the factors are not explicit and we cannot really interpret that well what representation the neural network has learned.\n",
    "\n",
    "\n",
    "#### How can you explain factor analysis to a high-school student?\n",
    "\n",
    "\"As a high school student, you have different subjects, and several exams in each subject. Imagine we keep track of the grades of each student in every exam, but forget to label which exam belongs to which subject. Now, we can expect that a certain student will tend to have similar grades in exams of the same subject, but may have very different grades in another subject. Thus, if we analyse in which exams most of the students seem to have grades that are more similar than average, we should be able to cluster the exams back into subjects. In this example, the subjects are what we call factors. We could even aim for a lower number of factors: in that case, we would probably end up putting all exams of similar subjects into the same bin.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
